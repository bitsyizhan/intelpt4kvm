diff -aNur linux-4.4.21-69-orig/arch/x86/include/asm/kvm_host.h linux-4.4.21-69-v13/arch/x86/include/asm/kvm_host.h
--- linux-4.4.21-69-orig/arch/x86/include/asm/kvm_host.h	2019-04-24 16:22:08.629273787 +0800
+++ linux-4.4.21-69-v13/arch/x86/include/asm/kvm_host.h	2019-04-25 18:43:53.921428576 +0800
@@ -866,6 +866,8 @@
 	void (*handle_external_intr)(struct kvm_vcpu *vcpu);
 	bool (*mpx_supported)(void);
 	bool (*xsaves_supported)(void);
+ 	bool (*umip_emulated)(void);						//pt
+	bool (*pt_supported)(void);						//pt
 
 	int (*check_nested_events)(struct kvm_vcpu *vcpu, bool external_intr);
 
diff -aNur linux-4.4.21-69-orig/arch/x86/include/asm/msr-index.h linux-4.4.21-69-v13/arch/x86/include/asm/msr-index.h
--- linux-4.4.21-69-orig/arch/x86/include/asm/msr-index.h	2019-04-24 16:22:08.629273787 +0800
+++ linux-4.4.21-69-v13/arch/x86/include/asm/msr-index.h	2019-04-26 17:48:18.033172198 +0800
@@ -88,11 +88,15 @@
 #define RTIT_CTL_CYCLEACC		BIT(1)
 #define RTIT_CTL_OS			BIT(2)
 #define RTIT_CTL_USR			BIT(3)
+#define RTIT_CTL_PWR_EVT_EN		BIT(4)						//pt
+#define RTIT_CTL_FUP_ON_PTW		BIT(5)						//pt
+#define RTIT_CTL_FABRIC_EN		BIT(6)						//pt
 #define RTIT_CTL_CR3EN			BIT(7)
 #define RTIT_CTL_TOPA			BIT(8)
 #define RTIT_CTL_MTC_EN			BIT(9)
 #define RTIT_CTL_TSC_EN			BIT(10)
 #define RTIT_CTL_DISRETC		BIT(11)
+#define RTIT_CTL_PTW_EN			BIT(12)						//pt
 #define RTIT_CTL_BRANCH_EN		BIT(13)
 #define RTIT_CTL_MTC_RANGE_OFFSET	14
 #define RTIT_CTL_MTC_RANGE		(0x0full << RTIT_CTL_MTC_RANGE_OFFSET)
@@ -100,11 +104,32 @@
 #define RTIT_CTL_CYC_THRESH		(0x0full << RTIT_CTL_CYC_THRESH_OFFSET)
 #define RTIT_CTL_PSB_FREQ_OFFSET	24
 #define RTIT_CTL_PSB_FREQ      		(0x0full << RTIT_CTL_PSB_FREQ_OFFSET)
+#define RTIT_CTL_ADDR0_OFFSET		32						//pt
+#define RTIT_CTL_ADDR0			(0x0full << RTIT_CTL_ADDR0_OFFSET)		//pt
+#define RTIT_CTL_ADDR1_OFFSET		36						//pt
+#define RTIT_CTL_ADDR1			(0x0full << RTIT_CTL_ADDR1_OFFSET)		//pt
+#define RTIT_CTL_ADDR2_OFFSET		40						//pt
+#define RTIT_CTL_ADDR2			(0x0full << RTIT_CTL_ADDR2_OFFSET)		//pt
+#define RTIT_CTL_ADDR3_OFFSET		44						//pt
+#define RTIT_CTL_ADDR3			(0x0full << RTIT_CTL_ADDR3_OFFSET)		//pt
 #define MSR_IA32_RTIT_STATUS		0x00000571
+#define MSR_IA32_RTIT_ADDR0_A           0x00000580					//pt
+#define MSR_IA32_RTIT_ADDR0_B           0x00000581					//pt
+#define MSR_IA32_RTIT_ADDR1_A           0x00000582					//pt
+#define MSR_IA32_RTIT_ADDR1_B           0x00000583					//pt
+#define MSR_IA32_RTIT_ADDR2_A           0x00000584					//pt
+#define MSR_IA32_RTIT_ADDR2_B           0x00000585					//pt
+#define MSR_IA32_RTIT_ADDR3_A           0x00000586					//pt
+#define MSR_IA32_RTIT_ADDR3_B           0x00000587					//pt
+#define RTIT_STATUS_FILTEREN		BIT(0)						//pt
 #define RTIT_STATUS_CONTEXTEN		BIT(1)
 #define RTIT_STATUS_TRIGGEREN		BIT(2)
+#define RTIT_STATUS_BUFFOVF		BIT(3)						//pt
 #define RTIT_STATUS_ERROR		BIT(4)
 #define RTIT_STATUS_STOPPED		BIT(5)
+#define RTIT_STATUS_BYTECNT_OFFSET	32						//pt
+#define RTIT_STATUS_BYTECNT		(0x1ffffull << RTIT_STATUS_BYTECNT_OFFSET)	//pt
+
 #define MSR_IA32_RTIT_CR3_MATCH		0x00000572
 #define MSR_IA32_RTIT_OUTPUT_BASE	0x00000560
 #define MSR_IA32_RTIT_OUTPUT_MASK	0x00000561
@@ -687,6 +712,7 @@
 #define VMX_BASIC_INOUT		0x0040000000000000LLU
 
 /* MSR_IA32_VMX_MISC bits */
+#define MSR_IA32_VMX_MISC_INTEL_PT                 (1ULL << 14)				//pt
 #define MSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS (1ULL << 29)
 #define MSR_IA32_VMX_MISC_PREEMPTION_TIMER_SCALE   0x1F
 /* AMD-V MSRs */
diff -aNur linux-4.4.21-69-orig/arch/x86/include/asm/vmx.h linux-4.4.21-69-v13/arch/x86/include/asm/vmx.h
--- linux-4.4.21-69-orig/arch/x86/include/asm/vmx.h	2019-04-24 16:22:08.745273789 +0800
+++ linux-4.4.21-69-v13/arch/x86/include/asm/vmx.h	2019-04-25 18:11:40.005380624 +0800
@@ -71,7 +71,9 @@
 #define SECONDARY_EXEC_ENABLE_INVPCID		0x00001000
 #define SECONDARY_EXEC_SHADOW_VMCS              0x00004000
 #define SECONDARY_EXEC_ENABLE_PML               0x00020000
+#define SECONDARY_EXEC_PT_CONCEAL_VMX		0x00080000			//pt
 #define SECONDARY_EXEC_XSAVES			0x00100000
+#define SECONDARY_EXEC_PT_USE_GPA		0x01000000			//pt
 #define SECONDARY_EXEC_PCOMMIT			0x00200000
 #define SECONDARY_EXEC_TSC_SCALING              0x02000000
 
@@ -93,6 +95,8 @@
 #define VM_EXIT_LOAD_IA32_EFER                  0x00200000
 #define VM_EXIT_SAVE_VMX_PREEMPTION_TIMER       0x00400000
 #define VM_EXIT_CLEAR_BNDCFGS                   0x00800000
+#define VM_EXIT_PT_CONCEAL_PIP			0x01000000			//pt
+#define VM_EXIT_CLEAR_IA32_RTIT_CTL		0x02000000			//pt
 
 #define VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR	0x00036dff
 
@@ -104,6 +108,8 @@
 #define VM_ENTRY_LOAD_IA32_PAT			0x00004000
 #define VM_ENTRY_LOAD_IA32_EFER                 0x00008000
 #define VM_ENTRY_LOAD_BNDCFGS                   0x00010000
+#define VM_ENTRY_PT_CONCEAL_PIP			0x00020000			//pt
+#define VM_ENTRY_LOAD_IA32_RTIT_CTL		0x00040000			//pt
 
 #define VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR	0x000011ff
 
@@ -192,6 +198,8 @@
 	GUEST_PDPTR3_HIGH               = 0x00002811,
 	GUEST_BNDCFGS                   = 0x00002812,
 	GUEST_BNDCFGS_HIGH              = 0x00002813,
+	GUEST_IA32_RTIT_CTL		= 0x00002814,					//pt
+	GUEST_IA32_RTIT_CTL_HIGH	= 0x00002815,					//pt
 	HOST_IA32_PAT			= 0x00002c00,
 	HOST_IA32_PAT_HIGH		= 0x00002c01,
 	HOST_IA32_EFER			= 0x00002c02,
diff -aNur linux-4.4.21-69-orig/arch/x86/kernel/cpu/intel_pt.h linux-4.4.21-69-v13/arch/x86/kernel/cpu/intel_pt.h
--- linux-4.4.21-69-orig/arch/x86/kernel/cpu/intel_pt.h	2019-04-24 16:22:08.821273791 +0800
+++ linux-4.4.21-69-v13/arch/x86/kernel/cpu/intel_pt.h	2019-04-26 17:21:03.341131665 +0800
@@ -48,15 +48,33 @@
 #define PT_CPUID_LEAVES		2
 #define PT_CPUID_REGS_NUM	4 /* number of regsters (eax, ebx, ecx, edx) */
 
+#define PT_MODE_SYSTEM		0				//pt
+#define PT_MODE_HOST_GUEST	1				//pt
+
+#define RTIT_ADDR_RANGE		4				//pt
+
+#define MSR_IA32_RTIT_STATUS_MASK (~(RTIT_STATUS_FILTEREN | \			//pt
+		RTIT_STATUS_CONTEXTEN | RTIT_STATUS_TRIGGEREN | \		//pt
+		RTIT_STATUS_ERROR | RTIT_STATUS_STOPPED | \			//pt
+		RTIT_STATUS_BYTECNT))						//pt
+
+#define MSR_IA32_RTIT_OUTPUT_BASE_MASK \					//pt
+		(~((1UL << cpuid_query_maxphyaddr(vcpu)) - 1) | 0x7f)		//pt
+
 enum pt_capabilities {
 	PT_CAP_max_subleaf = 0,
 	PT_CAP_cr3_filtering,
 	PT_CAP_psb_cyc,
+	PT_CAP_ip_filtering,			//pt
 	PT_CAP_mtc,
+	//PT_CAP_ptwrite,			//pt
+	//PT_CAP_power_event_trace,		//pt
 	PT_CAP_topa_output,
 	PT_CAP_topa_multiple_entries,
 	PT_CAP_single_range_output,
+	//PT_CAP_output_subsys,			//pt
 	PT_CAP_payloads_lip,
+	PT_CAP_num_address_ranges,		//pt
 	PT_CAP_mtc_periods,
 	PT_CAP_cycle_thresholds,
 	PT_CAP_psb_periods,
@@ -113,4 +131,14 @@
 	int			handle_nmi;
 };
 
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)					//pt
+void cpu_emergency_stop_pt(void);									//pt
+extern u32 intel_pt_validate_hw_cap(enum pt_capabilities cap);						//pt
+extern u32 intel_pt_validate_cap(u32 *caps, enum pt_capabilities cap);					//pt
+#else													//pt
+static inline void cpu_emergency_stop_pt(void) {}							//pt
+static inline u32 intel_pt_validate_hw_cap(enum pt_capabilities cap) { return 0; }			//pt
+static inline u32 intel_pt_validate_cap(u32 *caps, enum pt_capabilities capability) { return 0; }	//pt
+#endif
+
 #endif /* __INTEL_PT_H__ */
diff -aNur linux-4.4.21-69-orig/arch/x86/kernel/cpu/perf_event_intel_pt.c linux-4.4.21-69-v13/arch/x86/kernel/cpu/perf_event_intel_pt.c
--- linux-4.4.21-69-orig/arch/x86/kernel/cpu/perf_event_intel_pt.c	2019-04-24 16:22:08.829273792 +0800
+++ linux-4.4.21-69-v13/arch/x86/kernel/cpu/perf_event_intel_pt.c	2019-04-26 17:22:22.441133626 +0800
@@ -70,20 +70,31 @@
 	PT_CAP(topa_output,		0, CR_ECX, BIT(0)),
 	PT_CAP(topa_multiple_entries,	0, CR_ECX, BIT(1)),
 	PT_CAP(single_range_output,	0, CR_ECX, BIT(2)),
+	PT_CAP(output_subsys,		0, CPUID_ECX, BIT(3)),			//pt
 	PT_CAP(payloads_lip,		0, CR_ECX, BIT(31)),
 	PT_CAP(mtc_periods,		1, CR_EAX, 0xffff0000),
 	PT_CAP(cycle_thresholds,	1, CR_EBX, 0xffff),
 	PT_CAP(psb_periods,		1, CR_EBX, 0xffff0000),
 };
 
-static u32 pt_cap_get(enum pt_capabilities cap)
+//static u32 pt_cap_get(enum pt_capabilities cap)				//pt
+u32 intel_pt_validate_cap(u32 *caps, enum pt_capabilities cap)			//pt
 {
-	struct pt_cap_desc *cd = &pt_caps[cap];
-	u32 c = pt_pmu.caps[cd->leaf * PT_CPUID_REGS_NUM + cd->reg];
+	//struct pt_cap_desc *cd = &pt_caps[cap];				//pt
+	//u32 c = pt_pmu.caps[cd->leaf * PT_CPUID_REGS_NUM + cd->reg];		//pt
+	struct pt_cap_desc *cd = &pt_caps[capability];				//pt
+	u32 c = caps[cd->leaf * PT_CPUID_REGS_NUM + cd->reg];			//pt
 	unsigned int shift = __ffs(cd->mask);
 
 	return (c & cd->mask) >> shift;
 }
+EXPORT_SYMBOL_GPL(intel_pt_validate_cap);					//pt
+
+u32 intel_pt_validate_hw_cap(enum pt_capabilities cap)				//pt
+{										//pt
+	return intel_pt_validate_cap(pt_pmu.caps, cap);				//pt
+}										//pt
+EXPORT_SYMBOL_GPL(intel_pt_validate_hw_cap);					//pt
 
 static ssize_t pt_cap_show(struct device *cdev,
 			   struct device_attribute *attr,
@@ -93,7 +104,8 @@
 		container_of(attr, struct dev_ext_attribute, attr);
 	enum pt_capabilities cap = (long)ea->var;
 
-	return snprintf(buf, PAGE_SIZE, "%x\n", pt_cap_get(cap));
+	//return snprintf(buf, PAGE_SIZE, "%x\n", pt_cap_get(cap));			//pt
+	return snprintf(buf, PAGE_SIZE, "%x\n", intel_pt_validate_hw_cap(cap));		//pt
 }
 
 static struct attribute_group pt_cap_group = {
@@ -204,16 +216,19 @@
 		return false;
 
 	if (config & RTIT_CTL_CYC_PSB) {
-		if (!pt_cap_get(PT_CAP_psb_cyc))
+		//if (!pt_cap_get(PT_CAP_psb_cyc))				//pt
+		if (!intel_pt_validate_hw_cap(PT_CAP_psb_cyc))			//pt
 			return false;
 
-		allowed = pt_cap_get(PT_CAP_psb_periods);
+		//allowed = pt_cap_get(PT_CAP_psb_periods);			//pt
+		allowed = intel_pt_validate_hw_cap(PT_CAP_psb_periods);		//pt
 		requested = (config & RTIT_CTL_PSB_FREQ) >>
 			RTIT_CTL_PSB_FREQ_OFFSET;
 		if (requested && (!(allowed & BIT(requested))))
 			return false;
 
-		allowed = pt_cap_get(PT_CAP_cycle_thresholds);
+		//allowed = pt_cap_get(PT_CAP_cycle_thresholds);		//pt
+		allowed = intel_pt_validate_hw_cap(PT_CAP_cycle_thresholds);	//pt
 		requested = (config & RTIT_CTL_CYC_THRESH) >>
 			RTIT_CTL_CYC_THRESH_OFFSET;
 		if (requested && (!(allowed & BIT(requested))))
@@ -228,10 +243,12 @@
 		 * Spec says that setting mtc period bits while mtc bit in
 		 * CPUID is 0 will #GP, so better safe than sorry.
 		 */
-		if (!pt_cap_get(PT_CAP_mtc))
+		//if (!pt_cap_get(PT_CAP_mtc))					//pt
+		if (!intel_pt_validate_hw_cap(PT_CAP_mtc))			//pt
 			return false;
 
-		allowed = pt_cap_get(PT_CAP_mtc_periods);
+		//allowed = pt_cap_get(PT_CAP_mtc_periods);			//pt
+		allowed = intel_pt_validate_hw_cap(PT_CAP_mtc_periods);		//pt
 		if (!allowed)
 			return false;
 
@@ -359,7 +376,8 @@
 	 * In case of singe-entry ToPA, always put the self-referencing END
 	 * link as the 2nd entry in the table
 	 */
-	if (!pt_cap_get(PT_CAP_topa_multiple_entries)) {
+	//if (!pt_cap_get(PT_CAP_topa_multiple_entries)) {				//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries)) {			//pt
 		TOPA_ENTRY(topa, 1)->base = topa->phys >> TOPA_SHIFT;
 		TOPA_ENTRY(topa, 1)->end = 1;
 	}
@@ -399,7 +417,8 @@
 	topa->offset = last->offset + last->size;
 	buf->last = topa;
 
-	if (!pt_cap_get(PT_CAP_topa_multiple_entries))
+	//if (!pt_cap_get(PT_CAP_topa_multiple_entries))				//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))			//pt
 		return;
 
 	BUG_ON(last->last != TENTS_PER_PAGE - 1);
@@ -415,7 +434,8 @@
 static bool topa_table_full(struct topa *topa)
 {
 	/* single-entry ToPA is a special case */
-	if (!pt_cap_get(PT_CAP_topa_multiple_entries))
+	//if (!pt_cap_get(PT_CAP_topa_multiple_entries))			//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))		//pt
 		return !!topa->last;
 
 	return topa->last == TENTS_PER_PAGE - 1;
@@ -451,7 +471,9 @@
 
 	TOPA_ENTRY(topa, -1)->base = page_to_phys(p) >> TOPA_SHIFT;
 	TOPA_ENTRY(topa, -1)->size = order;
-	if (!buf->snapshot && !pt_cap_get(PT_CAP_topa_multiple_entries)) {
+	//if (!buf->snapshot && !pt_cap_get(PT_CAP_topa_multiple_entries)) {		//pt
+	if (!buf->snapshot &&								//pt
+	    !intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries)) {			//pt
 		TOPA_ENTRY(topa, -1)->intr = 1;
 		TOPA_ENTRY(topa, -1)->stop = 1;
 	}
@@ -486,7 +508,8 @@
 				 topa->table[i].intr ? 'I' : ' ',
 				 topa->table[i].stop ? 'S' : ' ',
 				 *(u64 *)&topa->table[i]);
-			if ((pt_cap_get(PT_CAP_topa_multiple_entries) &&
+			//if ((pt_cap_get(PT_CAP_topa_multiple_entries) &&		//pt
+			if ((intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries) &&	//pt
 			     topa->table[i].stop) ||
 			    topa->table[i].end)
 				break;
@@ -589,7 +612,8 @@
 		 * means we are already losing data; need to let the decoder
 		 * know.
 		 */
-		if (!pt_cap_get(PT_CAP_topa_multiple_entries) ||
+		//if (!pt_cap_get(PT_CAP_topa_multiple_entries) ||				//pt
+		if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries) ||			//pt
 		    buf->output_off == sizes(TOPA_ENTRY(buf->cur, buf->cur_idx)->size)) {
 			local_inc(&buf->lost);
 			advance++;
@@ -600,7 +624,9 @@
 	 * Also on single-entry ToPA implementations, interrupt will come
 	 * before the output reaches its output region's boundary.
 	 */
-	if (!pt_cap_get(PT_CAP_topa_multiple_entries) && !buf->snapshot &&
+	//if (!pt_cap_get(PT_CAP_topa_multiple_entries) && !buf->snapshot &&			//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries) &&				//pt
+	    !buf->snapshot &&									//pt
 	    pt_buffer_region_size(buf) - buf->output_off <= TOPA_PMI_MARGIN) {
 		void *head = pt_buffer_region(buf);
 
@@ -689,7 +715,8 @@
 
 
 	/* single entry ToPA is handled by marking all regions STOP=1 INT=1 */
-	if (!pt_cap_get(PT_CAP_topa_multiple_entries))
+	//if (!pt_cap_get(PT_CAP_topa_multiple_entries))			//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))		//pt
 		return 0;
 
 	/* clear STOP and INT from current entry */
@@ -840,7 +867,8 @@
 	pt_buffer_setup_topa_index(buf);
 
 	/* link last table to the first one, unless we're double buffering */
-	if (pt_cap_get(PT_CAP_topa_multiple_entries)) {
+	//if (pt_cap_get(PT_CAP_topa_multiple_entries)) {				//pt
+	if (intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries)) {			//pt
 		TOPA_ENTRY(buf->last, -1)->base = buf->first->phys >> TOPA_SHIFT;
 		TOPA_ENTRY(buf->last, -1)->end = 1;
 	}
@@ -1154,12 +1182,14 @@
 	if (ret)
 		return ret;
 
-	if (!pt_cap_get(PT_CAP_topa_output)) {
+	//if (!pt_cap_get(PT_CAP_topa_output)) {				//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_output)) {			//pt
 		pr_warn("ToPA output is not supported on this CPU\n");
 		return -ENODEV;
 	}
 
-	if (!pt_cap_get(PT_CAP_topa_multiple_entries))
+	//if (!pt_cap_get(PT_CAP_topa_multiple_entries))			//pt
+	if (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))		//pt
 		pt_pmu.pmu.capabilities =
 			PERF_PMU_CAP_AUX_NO_SG | PERF_PMU_CAP_AUX_SW_DOUBLEBUF;
 
diff -aNur linux-4.4.21-69-orig/arch/x86/kvm/cpuid.c linux-4.4.21-69-v13/arch/x86/kvm/cpuid.c
--- linux-4.4.21-69-orig/arch/x86/kvm/cpuid.c	2019-04-24 16:22:08.925273794 +0800
+++ linux-4.4.21-69-v13/arch/x86/kvm/cpuid.c	2019-04-25 18:56:49.677447811 +0800
@@ -297,6 +297,8 @@
 	unsigned f_invpcid = kvm_x86_ops->invpcid_supported() ? F(INVPCID) : 0;
 	unsigned f_mpx = kvm_x86_ops->mpx_supported() ? F(MPX) : 0;
 	unsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;
+ 	unsigned f_umip = kvm_x86_ops->umip_emulated() ? F(UMIP) : 0;				//pt
+	unsigned f_intel_pt = kvm_x86_ops->pt_supported() ? F(INTEL_PT) : 0;			//pt
 
 	/* cpuid 1.edx */
 	const u32 kvm_supported_word0_x86_features =
@@ -347,8 +349,9 @@
 	const u32 kvm_supported_word9_x86_features =
 		F(FSGSBASE) | F(BMI1) | F(HLE) | F(AVX2) | F(SMEP) |
 		F(BMI2) | F(ERMS) | f_invpcid | F(RTM) | f_mpx | F(RDSEED) |
-		F(ADX) | F(SMAP) | F(AVX512F) | F(AVX512PF) | F(AVX512ER) |
-		F(AVX512CD) | F(CLFLUSHOPT) | F(CLWB) | F(PCOMMIT);
+		F(ADX) | F(SMAP) | F(AVX512IFMA) | F(AVX512F) | F(AVX512PF) | 			//pt
+		F(AVX512ER) | F(AVX512CD) | F(CLFLUSHOPT) | F(CLWB) | F(AVX512DQ) | 		//pt
+		F(PCOMMIT) | F(SHA_NI) | F(AVX512BW) | F(AVX512VL) | f_intel_pt;		//pt
 
 	/* cpuid 0xD.1.eax */
 	const u32 kvm_supported_word10_x86_features =
@@ -367,7 +370,8 @@
 
 	switch (function) {
 	case 0:
-		entry->eax = min(entry->eax, (u32)0xd);
+		//entry->eax = min(entry->eax, (u32)0xd);				//pt
+		entry->eax = min(entry->eax, (u32)(f_intel_pt ? 0x14 : 0xd));		//pt
 		break;
 	case 1:
 		entry->edx &= kvm_supported_word0_x86_features;
@@ -530,6 +534,23 @@
 		}
 		break;
 	}
+	/* Intel PT */									//pt
+	case 0x14: {									//pt
+		int t, times = entry->eax;						//pt
+											//pt
+		if (!f_intel_pt)							//pt
+			break;								//pt
+											//pt
+		entry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;			//pt
+		for (t = 1; t <= times; ++t) {						//pt
+			if (*nent >= maxnent)						//pt
+				goto out;						//pt
+			do_cpuid_1_ent(&entry[t], function, t);				//pt
+			entry[t].flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;		//pt
+			++*nent;							//pt
+		}									//pt
+		break;									//pt
+	}										//pt
 	case KVM_CPUID_SIGNATURE: {
 		static const char signature[12] = "KVMKVMKVM\0\0";
 		const u32 *sigptr = (const u32 *)signature;
diff -aNur linux-4.4.21-69-orig/arch/x86/kvm/svm.c linux-4.4.21-69-v13/arch/x86/kvm/svm.c
--- linux-4.4.21-69-orig/arch/x86/kvm/svm.c	2019-04-24 16:22:08.929273794 +0800
+++ linux-4.4.21-69-v13/arch/x86/kvm/svm.c	2019-04-25 19:05:39.017460937 +0800
@@ -4055,6 +4055,16 @@
 	return false;
 }
 
+static bool svm_umip_emulated(void)				//pt
+{								//pt
+	return false;						//pt
+}								//pt
+
+static bool svm_pt_supported(void)				//pt
+{								//pt
+	return false;						//pt
+}								//pt
+
 static bool svm_has_wbinvd_exit(void)
 {
 	return true;
@@ -4346,6 +4356,8 @@
 	.invpcid_supported = svm_invpcid_supported,
 	.mpx_supported = svm_mpx_supported,
 	.xsaves_supported = svm_xsaves_supported,
+ 	.umip_emulated = svm_umip_emulated,				//pt
+	.pt_supported = svm_pt_supported,				//pt
 
 	.set_supported_cpuid = svm_set_supported_cpuid,
 
diff -aNur linux-4.4.21-69-orig/arch/x86/kvm/vmx.c linux-4.4.21-69-v13/arch/x86/kvm/vmx.c
--- linux-4.4.21-69-orig/arch/x86/kvm/vmx.c	2019-04-24 16:22:08.925273794 +0800
+++ linux-4.4.21-69-v13/arch/x86/kvm/vmx.c	2019-04-25 19:58:50.169540063 +0800
@@ -47,6 +47,7 @@
 #include <asm/kexec.h>
 #include <asm/apic.h>
 #include <asm/irq_remapping.h>
+#include <cpu/intel_pt.h>						//pt
 
 #include "trace.h"
 #include "pmu.h"
@@ -161,7 +162,12 @@
 static int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
 module_param(ple_window_max, int, S_IRUGO);
 
+/* Default is SYSTEM mode. */							//pt
+static int __read_mostly pt_mode = PT_MODE_SYSTEM;				//pt
+module_param(pt_mode, int, S_IRUGO);						//pt
+
 extern const ulong vmx_return;
+extern const ulong vmx_early_consistency_check_return;				//pt
 
 #define NR_AUTOLOAD_MSRS 8
 #define VMCS02_POOL_SIZE 1
@@ -517,6 +523,24 @@
 			(unsigned long *)&pi_desc->control);
 }
 
+struct pt_ctx {							//pt
+	u64 ctl;						//pt
+	u64 status;						//pt
+	u64 output_base;					//pt
+	u64 output_mask;					//pt
+	u64 cr3_match;						//pt
+	u64 addr_a[RTIT_ADDR_RANGE];				//pt
+	u64 addr_b[RTIT_ADDR_RANGE];				//pt
+};
+
+struct pt_desc {						//pt
+	u64 ctl_bitmask;					//pt
+	u32 addr_range;						//pt
+	u32 caps[PT_CPUID_REGS_NUM * PT_CPUID_LEAVES];		//pt
+	struct pt_ctx host;					//pt
+	struct pt_ctx guest;					//pt
+};								//pt
+
 struct vcpu_vmx {
 	struct kvm_vcpu       vcpu;
 	unsigned long         host_rsp;
@@ -597,6 +621,8 @@
 	struct page *pml_pg;
 
 	u64 current_tsc_ratio;
+
+	struct pt_desc pt_desc;				//pt
 };
 
 enum segment_cache_field {
@@ -877,6 +903,8 @@
 static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx);
 static int alloc_identity_pagetable(struct kvm *kvm);
 
+static void pt_set_intercept_for_msr(struct vcpu_vmx *vmx, bool flag);				//pt
+
 static DEFINE_PER_CPU(struct vmcs *, vmxarea);
 static DEFINE_PER_CPU(struct vmcs *, current_vmcs);
 /*
@@ -1182,6 +1210,20 @@
 		SECONDARY_EXEC_TSC_SCALING;
 }
 
+static inline bool cpu_has_vmx_intel_pt(void)					//pt
+{										//pt
+	u64 vmx_msr;								//pt
+										//pt
+	rdmsrl(MSR_IA32_VMX_MISC, vmx_msr);					//pt
+	return !!(vmx_msr & MSR_IA32_VMX_MISC_INTEL_PT);			//pt
+}										//pt
+
+static inline bool cpu_has_vmx_pt_use_gpa(void)					//pt
+{										//pt
+	return !!(vmcs_config.cpu_based_2nd_exec_ctrl &				//pt
+				SECONDARY_EXEC_PT_USE_GPA);			//pt
+}										//pt
+
 static inline bool report_flexpriority(void)
 {
 	return flexpriority_enabled;
@@ -2240,6 +2282,78 @@
 		vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);
 }
 
+static int vmx_rtit_ctl_check(struct kvm_vcpu *vcpu, u64 data)				//pt
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	unsigned long value;
+
+	/*
+	 * Any MSR write that attempts to change bits marked reserved will
+	 * case a #GP fault.
+	 */
+	if (data & vmx->pt_desc.ctl_bitmask)
+		return 1;
+
+	/*
+	 * Any attempt to modify IA32_RTIT_CTL while TraceEn is set will
+	 * result in a #GP unless the same write also clears TraceEn.
+	 */
+	if ((vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) &&
+		((vmx->pt_desc.guest.ctl ^ data) & ~RTIT_CTL_TRACEEN))
+		return 1;
+
+	/*
+	 * WRMSR to IA32_RTIT_CTL that sets TraceEn but clears this bit
+	 * and FabricEn would cause #GP, if
+	 * CPUID.(EAX=14H, ECX=0):ECX.SNGLRGNOUT[bit 2] = 0
+	 */
+	if ((data & RTIT_CTL_TRACEEN) && !(data & RTIT_CTL_TOPA) &&
+		!(data & RTIT_CTL_FABRIC_EN) &&
+		!intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_single_range_output))
+		return 1;
+
+	/*
+	 * MTCFreq, CycThresh and PSBFreq encodings check, any MSR write that
+	 * utilize encodings marked reserved will casue a #GP fault.
+	 */
+	value = intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc_periods);
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc) &&
+			!test_bit((data & RTIT_CTL_MTC_RANGE) >>
+			RTIT_CTL_MTC_RANGE_OFFSET, &value))
+		return 1;
+	value = intel_pt_validate_cap(vmx->pt_desc.caps,
+						PT_CAP_cycle_thresholds);
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc) &&
+			!test_bit((data & RTIT_CTL_CYC_THRESH) >>
+			RTIT_CTL_CYC_THRESH_OFFSET, &value))
+		return 1;
+	value = intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_periods);
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc) &&
+			!test_bit((data & RTIT_CTL_PSB_FREQ) >>
+			RTIT_CTL_PSB_FREQ_OFFSET, &value))
+		return 1;
+
+	/*
+	 * If ADDRx_CFG is reserved or the encodings is >2 will
+	 * cause a #GP fault.
+	 */
+	value = (data & RTIT_CTL_ADDR0) >> RTIT_CTL_ADDR0_OFFSET;
+	if ((value && (vmx->pt_desc.addr_range < 1)) || (value > 2))
+		return 1;
+	value = (data & RTIT_CTL_ADDR1) >> RTIT_CTL_ADDR1_OFFSET;
+	if ((value && (vmx->pt_desc.addr_range < 2)) || (value > 2))
+		return 1;
+	value = (data & RTIT_CTL_ADDR2) >> RTIT_CTL_ADDR2_OFFSET;
+	if ((value && (vmx->pt_desc.addr_range < 3)) || (value > 2))
+		return 1;
+	value = (data & RTIT_CTL_ADDR3) >> RTIT_CTL_ADDR3_OFFSET;
+	if ((value && (vmx->pt_desc.addr_range < 4)) || (value > 2))
+		return 1;
+
+	return 0;
+}
+
 static void skip_emulated_instruction(struct kvm_vcpu *vcpu)
 {
 	unsigned long rip;
@@ -2783,6 +2897,7 @@
 static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct shared_msr_entry *msr;
+	u32 index;					//pt
 
 	switch (msr_info->index) {
 #ifdef CONFIG_X86_64
@@ -2830,6 +2945,52 @@
 			return 1;
 		msr_info->data = vcpu->arch.ia32_xss;
 		break;
+	case MSR_IA32_RTIT_CTL:									//pt
+		if (pt_mode != PT_MODE_HOST_GUEST)
+			return 1;
+		msr_info->data = vmx->pt_desc.guest.ctl;
+		break;
+	case MSR_IA32_RTIT_STATUS:
+		if (pt_mode != PT_MODE_HOST_GUEST)
+			return 1;
+		msr_info->data = vmx->pt_desc.guest.status;
+		break;
+	case MSR_IA32_RTIT_CR3_MATCH:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			!intel_pt_validate_cap(vmx->pt_desc.caps,
+						PT_CAP_cr3_filtering))
+			return 1;
+		msr_info->data = vmx->pt_desc.guest.cr3_match;
+		break;
+	case MSR_IA32_RTIT_OUTPUT_BASE:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(!intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_topa_output) &&
+			 !intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_single_range_output)))
+			return 1;
+		msr_info->data = vmx->pt_desc.guest.output_base;
+		break;
+	case MSR_IA32_RTIT_OUTPUT_MASK:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(!intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_topa_output) &&
+			 !intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_single_range_output)))
+			return 1;
+		msr_info->data = vmx->pt_desc.guest.output_mask;
+		break;
+	case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:
+		index = msr_info->index - MSR_IA32_RTIT_ADDR0_A;
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(index >= 2 * intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_num_address_ranges)))
+			return 1;
+		if (index % 2)
+			msr_info->data = vmx->pt_desc.guest.addr_b[index / 2];
+		else
+			msr_info->data = vmx->pt_desc.guest.addr_a[index / 2];
+		break;										//pt
 	case MSR_TSC_AUX:
 		if (!guest_cpuid_has_rdtscp(vcpu) && !msr_info->host_initiated)
 			return 1;
@@ -2860,6 +3021,7 @@
 	int ret = 0;
 	u32 msr_index = msr_info->index;
 	u64 data = msr_info->data;
+	u32 index;						//pt
 
 	switch (msr_index) {
 	case MSR_EFER:
@@ -2936,6 +3098,64 @@
 		else
 			clear_atomic_switch_msr(vmx, MSR_IA32_XSS);
 		break;
+	case MSR_IA32_RTIT_CTL:									//pt
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||						//pt
+			//vmx_rtit_ctl_check(vcpu, data))					//pt
+			vmx_rtit_ctl_check(vcpu, data) ||					//pt
+			vmx->nested.vmxon)							//pt
+			return 1;
+		vmcs_write64(GUEST_IA32_RTIT_CTL, data);
+		pt_set_intercept_for_msr(vmx, !(data & RTIT_CTL_TRACEEN));			//pt
+		vmx->pt_desc.guest.ctl = data;
+		break;
+	case MSR_IA32_RTIT_STATUS:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) ||
+			(data & MSR_IA32_RTIT_STATUS_MASK))
+			return 1;
+		vmx->pt_desc.guest.status = data;
+		break;
+	case MSR_IA32_RTIT_CR3_MATCH:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) ||
+			!intel_pt_validate_cap(vmx->pt_desc.caps,
+						PT_CAP_cr3_filtering))
+			return 1;
+		vmx->pt_desc.guest.cr3_match = data;
+		break;
+	case MSR_IA32_RTIT_OUTPUT_BASE:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) ||
+			(!intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_topa_output) &&
+			 !intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_single_range_output)) ||
+			(data & MSR_IA32_RTIT_OUTPUT_BASE_MASK))
+			return 1;
+		vmx->pt_desc.guest.output_base = data;
+		break;
+	case MSR_IA32_RTIT_OUTPUT_MASK:
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) ||
+			(!intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_topa_output) &&
+			 !intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_single_range_output)))
+			return 1;
+		vmx->pt_desc.guest.output_mask = data;
+		break;
+	case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:
+		index = msr_info->index - MSR_IA32_RTIT_ADDR0_A;
+		if ((pt_mode != PT_MODE_HOST_GUEST) ||
+			(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) ||
+			(index >= 2 * intel_pt_validate_cap(vmx->pt_desc.caps,
+					PT_CAP_num_address_ranges)))
+			return 1;
+		if (index % 2)
+			vmx->pt_desc.guest.addr_b[index / 2] = data;
+		else
+			vmx->pt_desc.guest.addr_a[index / 2] = data;
+		break;										//pt
 	case MSR_TSC_AUX:
 		if (!guest_cpuid_has_rdtscp(vcpu) && !msr_info->host_initiated)
 			return 1;
@@ -2964,6 +3184,27 @@
 	return ret;
 }
 
+static void pt_set_intercept_for_msr(struct vcpu_vmx *vmx, bool flag)				//pt
+{
+	unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;
+	u32 i;
+
+	vmx_set_intercept_for_msr(msr_bitmap, MSR_IA32_RTIT_STATUS,
+							MSR_TYPE_RW, flag);
+	vmx_set_intercept_for_msr(msr_bitmap, MSR_IA32_RTIT_OUTPUT_BASE,
+							MSR_TYPE_RW, flag);
+	vmx_set_intercept_for_msr(msr_bitmap, MSR_IA32_RTIT_OUTPUT_MASK,
+							MSR_TYPE_RW, flag);
+	vmx_set_intercept_for_msr(msr_bitmap, MSR_IA32_RTIT_CR3_MATCH,
+							MSR_TYPE_RW, flag);
+	for (i = 0; i < vmx->pt_desc.addr_range; i++) {
+		vmx_set_intercept_for_msr(msr_bitmap,
+			MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+		vmx_set_intercept_for_msr(msr_bitmap,
+			MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	}
+}
+
 static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 {
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
@@ -3175,9 +3416,14 @@
 			SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
 			SECONDARY_EXEC_SHADOW_VMCS |
 			SECONDARY_EXEC_XSAVES |
+			SECONDARY_EXEC_RDRAND_EXITING |					//pt
 			SECONDARY_EXEC_ENABLE_PML |
 			SECONDARY_EXEC_PCOMMIT |
-			SECONDARY_EXEC_TSC_SCALING;
+			SECONDARY_EXEC_TSC_SCALING |					//pt
+			SECONDARY_EXEC_PT_USE_GPA |					//pt
+			SECONDARY_EXEC_PT_CONCEAL_VMX |					//pt
+			SECONDARY_EXEC_ENABLE_VMFUNC |					//pt
+ 			SECONDARY_EXEC_ENCLS_EXITING;					//pt
 		if (adjust_vmx_controls(min2, opt2,
 					MSR_IA32_VMX_PROCBASED_CTLS2,
 					&_cpu_based_2nd_exec_control) < 0)
@@ -3210,7 +3456,8 @@
 	min |= VM_EXIT_HOST_ADDR_SPACE_SIZE;
 #endif
 	opt = VM_EXIT_SAVE_IA32_PAT | VM_EXIT_LOAD_IA32_PAT |
-		VM_EXIT_ACK_INTR_ON_EXIT | VM_EXIT_CLEAR_BNDCFGS;
+		VM_EXIT_ACK_INTR_ON_EXIT | VM_EXIT_CLEAR_BNDCFGS |			//pt
+		VM_EXIT_PT_CONCEAL_PIP | VM_EXIT_CLEAR_IA32_RTIT_CTL;			//pt
 	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
 				&_vmexit_control) < 0)
 		return -EIO;
@@ -3227,11 +3474,21 @@
 		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
 
 	min = VM_ENTRY_LOAD_DEBUG_CONTROLS;
-	opt = VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS;
+	//opt = VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS;				//pt
+	opt = VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS |				//pt
+		VM_ENTRY_PT_CONCEAL_PIP | VM_ENTRY_LOAD_IA32_RTIT_CTL;
 	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
 				&_vmentry_control) < 0)
 		return -EIO;
 
+	if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_PT_USE_GPA) ||		//pt
+		!(_vmexit_control & VM_EXIT_CLEAR_IA32_RTIT_CTL) ||			//pt
+		!(_vmentry_control & VM_ENTRY_LOAD_IA32_RTIT_CTL)) {			//pt
+		_cpu_based_2nd_exec_control &= ~SECONDARY_EXEC_PT_USE_GPA;		//pt
+		_vmexit_control &= ~VM_EXIT_CLEAR_IA32_RTIT_CTL;			//pt
+		_vmentry_control &= ~VM_ENTRY_LOAD_IA32_RTIT_CTL;			//pt
+	}										//pt
+
 	rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);
 
 	/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */
@@ -4762,6 +5019,91 @@
 	kvm_mmu_set_mmio_spte_mask((0x3ull << 62) | 0x6ull);
 }
 
+static u32 vmx_vmexit_control(struct vcpu_vmx *vmx)				//pt
+{										//pt
+	u32 vmexit_control = vmcs_config.vmexit_ctrl;				//pt
+										//pt
+	if (pt_mode == PT_MODE_SYSTEM)						//pt
+		vmexit_control &= ~(VM_EXIT_CLEAR_IA32_RTIT_CTL |		//pt
+				    VM_EXIT_PT_CONCEAL_PIP);			//pt
+										//pt
+	return vmexit_control;							//pt
+}										//pt
+
+static u32 vmx_vmentry_control(struct vcpu_vmx *vmx)				//pt
+{										//pt
+	u32 vmentry_control = vmcs_config.vmentry_ctrl;				//pt
+										//pt
+	if (pt_mode == PT_MODE_SYSTEM)						//pt
+		vmentry_control &= ~(VM_ENTRY_PT_CONCEAL_PIP |			//pt
+				     VM_ENTRY_LOAD_IA32_RTIT_CTL);		//pt
+										//pt
+	return vmentry_control;							//pt
+}										//pt
+
+static inline void pt_load_msr(struct pt_ctx *ctx, u32 addr_range)		//pt
+{										//pt
+	u32 i;									//pt
+										//pt
+	wrmsrl(MSR_IA32_RTIT_STATUS, ctx->status);				//pt
+	wrmsrl(MSR_IA32_RTIT_OUTPUT_BASE, ctx->output_base);			//pt
+	wrmsrl(MSR_IA32_RTIT_OUTPUT_MASK, ctx->output_mask);			//pt
+	wrmsrl(MSR_IA32_RTIT_CR3_MATCH, ctx->cr3_match);			//pt
+	for (i = 0; i < addr_range; i++) {					//pt
+		wrmsrl(MSR_IA32_RTIT_ADDR0_A + i * 2, ctx->addr_a[i]);		//pt
+		wrmsrl(MSR_IA32_RTIT_ADDR0_B + i * 2, ctx->addr_b[i]);		//pt
+	}									//pt
+}										//pt
+
+static inline void pt_save_msr(struct pt_ctx *ctx, u32 addr_range)		//pt
+{										//pt
+	u32 i;									//pt
+										//pt
+	rdmsrl(MSR_IA32_RTIT_STATUS, ctx->status);				//pt
+	rdmsrl(MSR_IA32_RTIT_OUTPUT_BASE, ctx->output_base);			//pt
+	rdmsrl(MSR_IA32_RTIT_OUTPUT_MASK, ctx->output_mask);			//pt
+	rdmsrl(MSR_IA32_RTIT_CR3_MATCH, ctx->cr3_match);			//pt
+	for (i = 0; i < addr_range; i++) {					//pt
+		rdmsrl(MSR_IA32_RTIT_ADDR0_A + i * 2, ctx->addr_a[i]);		//pt
+		rdmsrl(MSR_IA32_RTIT_ADDR0_B + i * 2, ctx->addr_b[i]);		//pt
+	}									//pt
+}										//pt
+
+static void pt_guest_enter(struct vcpu_vmx *vmx)				//pt
+{										//pt
+	if (pt_mode == PT_MODE_SYSTEM)						//pt
+		return;								//pt
+										//pt
+	/* Save host state before VM entry */					//pt
+	rdmsrl(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);			//pt
+										//pt
+	/*									//pt
+	 * Set guest state of MSR_IA32_RTIT_CTL MSR (PT will be disabled	//pt
+	 * on VM entry when it has been disabled in guest before).		//pt
+	 */									//pt
+	vmcs_write64(GUEST_IA32_RTIT_CTL, vmx->pt_desc.guest.ctl);		//pt
+										//pt
+	if (vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) {			//pt
+		wrmsrl(MSR_IA32_RTIT_CTL, 0);					//pt
+		pt_save_msr(&vmx->pt_desc.host, vmx->pt_desc.addr_range);	//pt
+		pt_load_msr(&vmx->pt_desc.guest, vmx->pt_desc.addr_range);	//pt
+	}									//pt
+}										//pt
+
+static void pt_guest_exit(struct vcpu_vmx *vmx)					//pt
+{										//pt
+	if (pt_mode == PT_MODE_SYSTEM)						//pt
+		return;								//pt
+										//pt
+	if (vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) {			//pt
+		pt_save_msr(&vmx->pt_desc.guest, vmx->pt_desc.addr_range);	//pt
+		pt_load_msr(&vmx->pt_desc.host, vmx->pt_desc.addr_range);	//pt
+	}									//pt
+										//pt
+	/* Reload host state (IA32_RTIT_CTL will be cleared on VM exit). */	//pt
+	wrmsrl(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);			//pt
+}										//pt
+
 #define VMX_XSS_EXIT_BITMAP 0
 /*
  * Sets up the vmcs for emulated real mode.
@@ -4854,11 +5196,12 @@
 		++vmx->nmsrs;
 	}
 
-
-	vm_exit_controls_init(vmx, vmcs_config.vmexit_ctrl);
+	//vm_exit_controls_init(vmx, vmcs_config.vmexit_ctrl);			//pt
+	vm_exit_controls_init(vmx, vmx_vmexit_control(vmx));			//pt
 
 	/* 22.2.1, 20.8.1 */
-	vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);
+	//vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);		//pt
+	vm_entry_controls_init(vmx, vmx_vmentry_control(vmx));			//pt
 
 	vmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);
 	set_cr4_guest_host_mask(vmx);
@@ -4866,6 +5209,13 @@
 	if (vmx_xsaves_supported())
 		vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);
 
+	if (pt_mode == PT_MODE_HOST_GUEST) {					//pt
+		memset(&vmx->pt_desc, 0, sizeof(vmx->pt_desc));			//pt
+		/* Bit[6~0] are forced to 1, writes are ignored. */		//pt
+		vmx->pt_desc.guest.output_mask = 0x7F;				//pt
+		vmcs_write64(GUEST_IA32_RTIT_CTL, 0);				//pt
+	}									//pt
+
 	return 0;
 }
 
@@ -6301,6 +6651,9 @@
 
 	kvm_set_posted_intr_wakeup_handler(wakeup_handler);
 
+	if (!enable_ept || !cpu_has_vmx_intel_pt() || !cpu_has_vmx_pt_use_gpa())		//pt
+		pt_mode = PT_MODE_SYSTEM;							//pt
+
 	return alloc_kvm_area();
 
 out8:
@@ -6794,6 +7147,11 @@
 
 	vmx->nested.vmxon = true;
 
+	if (pt_mode == PT_MODE_HOST_GUEST) {					//pt
+		vmx->pt_desc.guest.ctl = 0;					//pt
+		pt_set_intercept_for_msr(vmx, 1);				//pt
+	}									//pt
+
 	skip_emulated_instruction(vcpu);
 	nested_vmx_succeed(vcpu);
 	return 1;
@@ -8393,6 +8751,16 @@
 		SECONDARY_EXEC_XSAVES;
 }
 
+static bool vmx_pt_supported(void)				//pt
+{								//pt
+	return false;						//pt
+}								//pt
+
+static bool vmx_pt_supported(void)				//pt
+{								//pt
+	return (pt_mode == PT_MODE_HOST_GUEST);			//pt
+}								//pt
+
 static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 {
 	u32 exit_intr_info;
@@ -8567,6 +8935,8 @@
 	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
 		vmx_set_interrupt_shadow(vcpu, 0);
 
+	pt_guest_enter(vmx);					//pt
+
 	atomic_switch_perf_msrs(vmx);
 	debugctlmsr = get_debugctlmsr();
 
@@ -8700,6 +9070,8 @@
 				  | (1 << VCPU_EXREG_CR3));
 	vcpu->arch.regs_dirty = 0;
 
+	pt_guest_exit(vmx);					//pt
+
 	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
 
 	vmx->loaded_vmcs->launched = 1;
@@ -8950,6 +9322,75 @@
 		     (new_ctl & ~mask) | (cur_ctl & mask));
 }
 
+static void update_intel_pt_cfg(struct kvm_vcpu *vcpu)						//pt
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	struct kvm_cpuid_entry2 *best = NULL;
+	int i;
+
+	for (i = 0; i < PT_CPUID_LEAVES; i++) {
+		best = kvm_find_cpuid_entry(vcpu, 0x14, i);
+		if (!best)
+			return;
+		vmx->pt_desc.caps[CPUID_EAX + i*PT_CPUID_REGS_NUM] = best->eax;
+		vmx->pt_desc.caps[CPUID_EBX + i*PT_CPUID_REGS_NUM] = best->ebx;
+		vmx->pt_desc.caps[CPUID_ECX + i*PT_CPUID_REGS_NUM] = best->ecx;
+		vmx->pt_desc.caps[CPUID_EDX + i*PT_CPUID_REGS_NUM] = best->edx;
+	}
+
+	/* Get the number of configurable Address Ranges for filtering */
+	vmx->pt_desc.addr_range = intel_pt_validate_cap(vmx->pt_desc.caps,
+						PT_CAP_num_address_ranges);
+
+	/* Initialize and clear the no dependency bits */
+	vmx->pt_desc.ctl_bitmask = ~(RTIT_CTL_TRACEEN | RTIT_CTL_OS |
+			RTIT_CTL_USR | RTIT_CTL_TSC_EN | RTIT_CTL_DISRETC);
+
+	/*
+	 * If CPUID.(EAX=14H,ECX=0):EBX[0]=1 CR3Filter can be set otherwise
+	 * will inject an #GP
+	 */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_cr3_filtering))
+		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_CR3EN;
+
+	/*
+	 * If CPUID.(EAX=14H,ECX=0):EBX[1]=1 CYCEn, CycThresh and
+	 * PSBFreq can be set
+	 */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc))
+		vmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_CYCLEACC |
+				RTIT_CTL_CYC_THRESH | RTIT_CTL_PSB_FREQ);
+
+	/*
+	 * If CPUID.(EAX=14H,ECX=0):EBX[3]=1 MTCEn BranchEn and
+	 * MTCFreq can be set
+	 */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc))
+		vmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_MTC_EN |
+				RTIT_CTL_BRANCH_EN | RTIT_CTL_MTC_RANGE);
+
+	/* If CPUID.(EAX=14H,ECX=0):EBX[4]=1 FUPonPTW and PTWEn can be set */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_ptwrite))
+		vmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_FUP_ON_PTW |
+							RTIT_CTL_PTW_EN);
+
+	/* If CPUID.(EAX=14H,ECX=0):EBX[5]=1 PwrEvEn can be set */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_power_event_trace))
+		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_PWR_EVT_EN;
+
+	/* If CPUID.(EAX=14H,ECX=0):ECX[0]=1 ToPA can be set */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_topa_output))
+		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_TOPA;
+
+	/* If CPUID.(EAX=14H,ECX=0):ECX[3]=1 FabircEn can be set */
+	if (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_output_subsys))
+		vmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_FABRIC_EN;
+
+	/* unmask address range configure area */
+	for (i = 0; i < vmx->pt_desc.addr_range; i++)
+		vmx->pt_desc.ctl_bitmask &= ~(0xf << (32 + i * 4));
+}
+
 static void vmx_cpuid_update(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpuid_entry2 *best;
@@ -8993,6 +9434,9 @@
 			vmx->nested.nested_vmx_secondary_ctls_high &=
 				~SECONDARY_EXEC_PCOMMIT;
 	}
+	if (boot_cpu_has(X86_FEATURE_INTEL_PT) &&					//pt
+			guest_cpuid_has(vcpu, X86_FEATURE_INTEL_PT))			//pt
+		update_intel_pt_cfg(vcpu);						//pt
 }
 
 static void vmx_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
@@ -10891,6 +11335,8 @@
 	.handle_external_intr = vmx_handle_external_intr,
 	.mpx_supported = vmx_mpx_supported,
 	.xsaves_supported = vmx_xsaves_supported,
+ 	.umip_emulated = vmx_umip_emulated,				//pt
+	.pt_supported = vmx_pt_supported,				//pt
 
 	.check_nested_events = vmx_check_nested_events,
 
diff -aNur linux-4.4.21-69-orig/arch/x86/kvm/x86.c linux-4.4.21-69-v13/arch/x86/kvm/x86.c
--- linux-4.4.21-69-orig/arch/x86/kvm/x86.c	2019-04-24 16:22:08.925273794 +0800
+++ linux-4.4.21-69-v13/arch/x86/kvm/x86.c	2019-04-25 19:48:21.385524472 +0800
@@ -54,6 +54,7 @@
 #include <linux/kvm_irqfd.h>
 #include <linux/irqbypass.h>
 #include <trace/events/kvm.h>
+#include <cpu/intel_pt.h>						//pt
 
 #define CREATE_TRACE_POINTS
 #include "trace.h"
@@ -951,6 +952,13 @@
 #endif
 	MSR_IA32_TSC, MSR_IA32_CR_PAT, MSR_VM_HSAVE_PA,
 	MSR_IA32_FEATURE_CONTROL, MSR_IA32_BNDCFGS, MSR_TSC_AUX,
+	MSR_IA32_SPEC_CTRL, MSR_IA32_ARCH_CAPABILITIES,				//pt
+	MSR_IA32_RTIT_CTL, MSR_IA32_RTIT_STATUS, MSR_IA32_RTIT_CR3_MATCH,	//pt
+	MSR_IA32_RTIT_OUTPUT_BASE, MSR_IA32_RTIT_OUTPUT_MASK,			//pt
+	MSR_IA32_RTIT_ADDR0_A, MSR_IA32_RTIT_ADDR0_B,				//pt
+	MSR_IA32_RTIT_ADDR1_A, MSR_IA32_RTIT_ADDR1_B,				//pt
+	MSR_IA32_RTIT_ADDR2_A, MSR_IA32_RTIT_ADDR2_B,				//pt
+	MSR_IA32_RTIT_ADDR3_A, MSR_IA32_RTIT_ADDR3_B,				//pt
 };
 
 static unsigned num_msrs_to_save;
@@ -4022,6 +4030,30 @@
 			if (!kvm_x86_ops->rdtscp_supported())
 				continue;
 			break;
+		case MSR_IA32_RTIT_CTL:								//pt
+		case MSR_IA32_RTIT_STATUS:
+			if (!kvm_x86_ops->pt_supported())
+				continue;
+			break;
+		case MSR_IA32_RTIT_CR3_MATCH:
+			if (!kvm_x86_ops->pt_supported() ||
+			    !intel_pt_validate_hw_cap(PT_CAP_cr3_filtering))
+				continue;
+			break;
+		case MSR_IA32_RTIT_OUTPUT_BASE:
+		case MSR_IA32_RTIT_OUTPUT_MASK:
+			if (!kvm_x86_ops->pt_supported() ||
+				(!intel_pt_validate_hw_cap(PT_CAP_topa_output) &&
+				 !intel_pt_validate_hw_cap(PT_CAP_single_range_output)))
+				continue;
+			break;
+		case MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B: {
+			if (!kvm_x86_ops->pt_supported() ||
+				msrs_to_save[i] - MSR_IA32_RTIT_ADDR0_A >=
+				intel_pt_validate_hw_cap(PT_CAP_num_address_ranges) * 2)
+				continue;
+			break;
+		}										//pt
 		default:
 			break;
 		}
